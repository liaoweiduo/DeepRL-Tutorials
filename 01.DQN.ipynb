{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "import math\n",
    "from utils.wrappers import make_atari, wrap_deepmind, wrap_pytorch\n",
    "\n",
    "from utils.hyperparameters import Config\n",
    "from agents.BaseAgent import BaseAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "\n",
    "config.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#epsilon variables\n",
    "config.epsilon_start = 1.0\n",
    "config.epsilon_final = 0.01\n",
    "config.epsilon_decay = 30000\n",
    "config.epsilon_by_frame = lambda frame_idx: config.epsilon_final + (config.epsilon_start - config.epsilon_final) * math.exp(-1. * frame_idx / config.epsilon_decay)\n",
    "\n",
    "#misc agent variables\n",
    "config.GAMMA=0.99\n",
    "config.LR=1e-4\n",
    "\n",
    "#memory\n",
    "config.TARGET_NET_UPDATE_FREQ = 1000\n",
    "config.EXP_REPLAY_SIZE = 100000\n",
    "config.BATCH_SIZE = 32\n",
    "\n",
    "#Learning control variables\n",
    "config.LEARN_START = 10000\n",
    "config.MAX_FRAMES=1000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "\n",
    "    def push(self, transition):\n",
    "        self.memory.append(transition)\n",
    "        if len(self.memory) > self.capacity:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):   # input:3*10 actions:105\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "        # self.conv1 = nn.Conv2d(self.input_shape[0], 32, kernel_size=8, stride=4)\n",
    "        # self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        # self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "\n",
    "        self.hidden1 = nn.Linear(self.input_shape[0] * self.input_shape[1], 61)\n",
    "        self.hidden2 = nn.Linear(61, 211)\n",
    "        self.predict = nn.Linear(211, self.num_actions)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)   # 拉长成向量 第一维是同时处理的数据数 不变，后面拉长\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    # def feature_size(self):\n",
    "    #     return self.conv3(self.conv2(self.conv1(torch.zeros(1, *self.input_shape)))).view(1, -1).size(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(BaseAgent):\n",
    "    def __init__(self, static_policy=False, env=None, config=None):\n",
    "        super(Model, self).__init__()\n",
    "        self.device = config.device\n",
    "\n",
    "        self.gamma = config.GAMMA\n",
    "        self.lr = config.LR\n",
    "        self.target_net_update_freq = config.TARGET_NET_UPDATE_FREQ\n",
    "        self.experience_replay_size = config.EXP_REPLAY_SIZE\n",
    "        self.batch_size = config.BATCH_SIZE\n",
    "        self.learn_start = config.LEARN_START\n",
    "\n",
    "        self.static_policy = static_policy\n",
    "        self.num_feats = env.observation_space.shape\n",
    "        self.num_actions = env.action_space.n\n",
    "        self.env = env\n",
    "\n",
    "        self.declare_networks()\n",
    "            \n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        \n",
    "        #move to correct device\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.target_model.to(self.device)\n",
    "\n",
    "        if self.static_policy:\n",
    "            self.model.eval()\n",
    "            self.target_model.eval()\n",
    "        else:\n",
    "            self.model.train()\n",
    "            self.target_model.train()\n",
    "\n",
    "        self.update_count = 0\n",
    "\n",
    "        self.declare_memory()\n",
    "        \n",
    "\n",
    "    def declare_networks(self):\n",
    "        self.model = DQN(self.num_feats, self.num_actions)\n",
    "        self.target_model = DQN(self.num_feats, self.num_actions)\n",
    "\n",
    "    def declare_memory(self):\n",
    "        self.memory = ExperienceReplayMemory(self.experience_replay_size)\n",
    "\n",
    "    def append_to_replay(self, s, a, r, s_):\n",
    "        self.memory.push((s, a, r, s_))\n",
    "\n",
    "\n",
    "    def prep_minibatch(self):\n",
    "        # random transition batch is taken from experience replay memory\n",
    "        transitions = self.memory.sample(self.batch_size)\n",
    "        \n",
    "        batch_state, batch_action, batch_reward, batch_next_state = zip(*transitions)\n",
    "\n",
    "        shape = (-1,)+self.num_feats\n",
    "\n",
    "        batch_state = torch.tensor(batch_state, device=self.device, dtype=torch.float).view(shape)\n",
    "        batch_action = torch.tensor(batch_action, device=self.device, dtype=torch.long).squeeze().view(-1, 1)\n",
    "        batch_reward = torch.tensor(batch_reward, device=self.device, dtype=torch.float).squeeze().view(-1, 1)\n",
    "        \n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch_next_state)), device=self.device, dtype=torch.uint8)\n",
    "        try: #sometimes all next states are false\n",
    "            non_final_next_states = torch.tensor([s for s in batch_next_state if s is not None], device=self.device, dtype=torch.float).view(shape)\n",
    "            empty_next_state_values = False\n",
    "        except:\n",
    "            non_final_next_states = None\n",
    "            empty_next_state_values = True\n",
    "\n",
    "        return batch_state, batch_action, batch_reward, non_final_next_states, non_final_mask, empty_next_state_values\n",
    "\n",
    "    def compute_loss(self, batch_vars):\n",
    "        batch_state, batch_action, batch_reward, non_final_next_states, non_final_mask, empty_next_state_values = batch_vars\n",
    "\n",
    "        #estimate\n",
    "        current_q_values = self.model(batch_state).gather(1, batch_action)\n",
    "        \n",
    "        #target\n",
    "        with torch.no_grad():\n",
    "            max_next_q_values = torch.zeros(self.batch_size, device=self.device, dtype=torch.float).unsqueeze(dim=1)\n",
    "            if not empty_next_state_values:\n",
    "                max_next_action = self.get_max_next_state_action(non_final_next_states)\n",
    "                max_next_q_values[non_final_mask] = self.target_model(non_final_next_states).gather(1, max_next_action)\n",
    "            expected_q_values = batch_reward + (self.gamma*max_next_q_values)\n",
    "\n",
    "        diff = (expected_q_values - current_q_values)\n",
    "        loss = self.huber(diff)\n",
    "        loss = loss.mean()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def update(self, s, a, r, s_, frame=0):\n",
    "        if self.static_policy:\n",
    "            return None\n",
    "\n",
    "        self.append_to_replay(s, a, r, s_)\n",
    "\n",
    "        if frame < self.learn_start:\n",
    "            return None\n",
    "\n",
    "        batch_vars = self.prep_minibatch()\n",
    "\n",
    "        loss = self.compute_loss(batch_vars)\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in self.model.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.update_target_model()\n",
    "        self.save_loss(loss.item())\n",
    "        self.save_sigma_param_magnitudes()\n",
    "\n",
    "\n",
    "    def get_action(self, s, eps=0.1):\n",
    "        with torch.no_grad():\n",
    "            if np.random.random() >= eps or self.static_policy:\n",
    "                X = torch.tensor([s], device=self.device, dtype=torch.float)\n",
    "                a = self.model(X).max(1)[1].view(1, 1)\n",
    "                return a.item()\n",
    "            else:\n",
    "                return np.random.randint(0, self.num_actions)\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.update_count+=1\n",
    "        self.update_count = self.update_count % self.target_net_update_freq\n",
    "        if self.update_count == 0:\n",
    "            self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def get_max_next_state_action(self, next_states):\n",
    "        return self.target_model(next_states).max(dim=1)[1].view(-1, 1)\n",
    "\n",
    "    def huber(self, x):\n",
    "        cond = (x.abs() < 1.0).to(torch.float)\n",
    "        return 0.5 * x.pow(2) * cond + (x.abs() - 0.5) * (1 - cond)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(frame_idx, rewards, losses, sigma, elapsed_time):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('frame %s. reward: %s. time: %s' % (frame_idx, np.mean(rewards[-10:]), elapsed_time))\n",
    "    plt.plot(rewards)\n",
    "    if losses:\n",
    "        plt.subplot(132)\n",
    "        plt.title('loss')\n",
    "        plt.plot(losses)\n",
    "    if sigma:\n",
    "        plt.subplot(133)\n",
    "        plt.title('noisy param magnitude')\n",
    "        plt.plot(sigma)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start=timer()\n",
    "#\n",
    "# env_id = \"PongNoFrameskip-v4\"\n",
    "# env    = make_atari(env_id)\n",
    "# env    = wrap_deepmind(env, frame_stack=False)\n",
    "# env    = wrap_pytorch(env)\n",
    "# model = Model(env=env, config=config)\n",
    "#\n",
    "# episode_reward = 0\n",
    "#\n",
    "# observation = env.reset()\n",
    "# for frame_idx in range(1, config.MAX_FRAMES + 1):\n",
    "#     epsilon = config.epsilon_by_frame(frame_idx)\n",
    "#\n",
    "#     action = model.get_action(observation, epsilon)\n",
    "#     prev_observation=observation\n",
    "#     observation, reward, done, _ = env.step(action)\n",
    "#     observation = None if done else observation\n",
    "#\n",
    "#     model.update(prev_observation, action, reward, observation, frame_idx)\n",
    "#     episode_reward += reward\n",
    "#\n",
    "#     if done:\n",
    "#         observation = env.reset()\n",
    "#         model.save_reward(episode_reward)\n",
    "#         episode_reward = 0\n",
    "#\n",
    "#         if np.mean(model.rewards[-10:]) > 19:\n",
    "#             plot(frame_idx, model.rewards, model.losses, model.sigma_parameter_mag, timedelta(seconds=int(timer()-start)))\n",
    "#             break\n",
    "#\n",
    "#     if frame_idx % 10000 == 0:\n",
    "#         plot(frame_idx, model.rewards, model.losses, model.sigma_parameter_mag, timedelta(seconds=int(timer()-start)))\n",
    "#\n",
    "# model.save_w()\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Calculated padded input size per channel: (5 x 1). Kernel size: (4 x 4). Kernel size can't be greater than actual input size",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-11-e9f7379523b6>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[1;31m# env    = wrap_deepmind(env, frame_stack=False)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[0menv\u001B[0m    \u001B[1;33m=\u001B[0m \u001B[0mwrap_pytorch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0menv\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 8\u001B[1;33m \u001B[0mmodel\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mModel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0menv\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0menv\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      9\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[1;31m# episode_reward = 0\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-5-f3302564dc8e>\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, static_policy, env, config)\u001B[0m\n\u001B[0;32m     16\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0menv\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0menv\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 18\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdeclare_networks\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     19\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     20\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtarget_model\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload_state_dict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstate_dict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-5-f3302564dc8e>\u001B[0m in \u001B[0;36mdeclare_networks\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     38\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     39\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mdeclare_networks\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 40\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmodel\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mDQN\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnum_feats\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnum_actions\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     41\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtarget_model\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mDQN\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnum_feats\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnum_actions\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     42\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-4-b0ec5bc3ba4d>\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, input_shape, num_actions)\u001B[0m\n\u001B[0;32m     10\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconv3\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mConv2d\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m64\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m64\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkernel_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m3\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstride\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 12\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfc1\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mLinear\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfeature_size\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m512\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     13\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfc2\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mLinear\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m512\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnum_actions\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-4-b0ec5bc3ba4d>\u001B[0m in \u001B[0;36mfeature_size\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     24\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     25\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mfeature_size\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 26\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconv3\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconv2\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconv1\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mzeros\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minput_shape\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mview\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     27\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\weiduliao2\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m    720\u001B[0m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    721\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 722\u001B[1;33m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    723\u001B[0m         for hook in itertools.chain(\n\u001B[0;32m    724\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\weiduliao2\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    417\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    418\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mTensor\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[0mTensor\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 419\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_conv_forward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mweight\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    420\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    421\u001B[0m \u001B[1;32mclass\u001B[0m \u001B[0mConv3d\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0m_ConvNd\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\weiduliao2\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001B[0m in \u001B[0;36m_conv_forward\u001B[1;34m(self, input, weight)\u001B[0m\n\u001B[0;32m    414\u001B[0m                             _pair(0), self.dilation, self.groups)\n\u001B[0;32m    415\u001B[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001B[1;32m--> 416\u001B[1;33m                         self.padding, self.dilation, self.groups)\n\u001B[0m\u001B[0;32m    417\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    418\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mTensor\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[0mTensor\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Calculated padded input size per channel: (5 x 1). Kernel size: (4 x 4). Kernel size can't be greater than actual input size"
     ]
    }
   ],
   "source": [
    "start=timer()\n",
    "\n",
    "# env_id = \"PongNoFrameskip-v4\"\n",
    "env_id = \"gym_DVSinR2HCA:DVSinR2HCA-v0\"\n",
    "env = gym.make(env_id)\n",
    "model = Model(env=env, config=config)\n",
    "\n",
    "# episode_reward = 0\n",
    "\n",
    "observation = env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(1, 21, 5)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 105 into shape (1,1,1)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-10-59763efc3058>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mt\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mobservation\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreshape\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mobservation\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcopy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m: cannot reshape array of size 105 into shape (1,1,1)"
     ]
    }
   ],
   "source": [
    "# t = observation.reshape((observation.copy().shape[0],1,1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}